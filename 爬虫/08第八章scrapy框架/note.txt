scrapy框架

-什么是框架？
    -就是一个集成了很多功能并且有很强通用性的一个项目模板
-如何学习框架？
    -专门学习框架封装的各种功能的详细用法。
-什么是scrapy?
    -爬虫中封装好的一个明星框架。
    功能：高性能的持久化存储，异步的数据下载，高性能的数据解析，分布式
-scrapy框架的基本使用
    -环境的安装：
    -mac or linux: pip install scrapy
    -windows:
        -pip install wheel
    -下载twisted,下载地址为http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted
    -安装twisted: pip install Twisted-17.1.0-cp36-cp36m-win_amd64.whl
        -pip install pvwin32
        -pip install scrapy
        测试:在终端里录入scrapy指令，没有报错即表示安装成功!
        cd 爬虫/08第八章scrapy框架/
    -创建一个工程：scrapy startproject scrapyproject
        -在spiders子目录下创建一个爬虫文件
        cd scrapyproject
        -scrapy genspider first www.xxx.com
    -执行工程：
        -scrapy crawl spiderName
        -scrapy crawl first --nolog可以去除日志
        #显示指定类型的日志信息
        LOG_LEVEL = 'ERROR'
   responsed的属性和方法
        response.text  #获取的是响应的字符串
        response。body #获取的是二进制数据
        response.xpath #可以直接xpath方法来解析response中的内容
        response.extract() 提取seletor对象的data属性值
        response.extract_first() 提取的seletor列表的第一个数据
-scrapy持久化存储
    -基于终端指令：
        -要求：只可以将parse方法的返回值存储到本地的文本文件中
        -注意；持久化存储对应的文本文件类型只可以为：json，jsonlines，csv，xml，marshal，pickle
        -指令：scrapy crawl xxx -o filePath
        -好处：简洁高效便捷
        -缺点：局限性比较强（数据只可以存到指定后缀的文本文件中
    -基于管道：
        -编码流程：
            -数据解析
            -在items类中定义相关的属性
            -将解析的数据封装存储到item类型的对象
            -将item类型的对象提交给管道进行持久化存储的操作
            -在管道类的process_item中将其接受到的item对象中存储的数据进行持久化存储操作
            -在配置文件中开启管道
        -好处：通用性强
    -面试题：将爬取到的数据一份存储到本地一份存储到数据库，如何实现？
    -管道文件中一个管道类对应的是将数据存储到一种平台
    -爬虫文件提交的item只会给管道文件中第一个被执行的管道类接受
    -process_items中国return中的return items 表示将item传递给下一个即将被执行的管道类

基于Spider的全站数据爬取
    -就是将网站中某板块下的全部页码对应的页面数据进行爬取
    需求：爬取小说网站中的所有章节的名称
    实现方式：
        将所有页面的url添加到start_urls列表（不推荐）
        自行手动进行请求发送（推荐）
            手动发送请求：
                yield scrapy.Request(url,callback):callback专门用做于数据解析
scrapy五大核心组件：
    spider，调度器，引擎，管道，下载器

请求传参：
    -使用场景：如果爬取解析的数据不在同一张页面中。（深度爬取）

